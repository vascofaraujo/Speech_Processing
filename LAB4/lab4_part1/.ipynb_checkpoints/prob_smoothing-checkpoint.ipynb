{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34ee1c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vasco/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, flatten\n",
    "from nltk.lm import MLE, Laplace\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ead9fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_txt = 'utf8_s1.txt'\n",
    "\n",
    "with open(new_file_txt, 'r') as file:\n",
    "    data = file.read().replace('<s>', '').replace('</s>','').replace('\\n', '')\n",
    "    \n",
    "bigrams = list(nltk.ngrams(data.split(), n=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b61c88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(data)\n",
    "words = nltk.word_tokenize(data)\n",
    "\n",
    "bigramList = []\n",
    "\n",
    "for sentence in sents:\n",
    "    sequence = word_tokenize(sentence)\n",
    "    bigramList.extend(list(nltk.ngrams(sequence,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d580162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411299 207567\n"
     ]
    }
   ],
   "source": [
    "#total number of bigrams\n",
    "N = len(bigramList)\n",
    "#number of unique bigrams\n",
    "V = len(set(bigramList))\n",
    "\n",
    "print(N,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b6c2d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, bigramList)\n",
    "\n",
    "model = Laplace(n)  #laplace model implements add 1 smoothing\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52d8d311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ele', 'sofreu', 'uma', 'queda', 'no', 'primeiro', 'jogo', '.']\n"
     ]
    }
   ],
   "source": [
    "inputSentence = \"s2.txt\"\n",
    "with open(inputSentence, 'r') as file:\n",
    "    data = file.read().replace('<s>', '').replace('</s>','').replace('\\n', '')\n",
    "\n",
    "words = nltk.word_tokenize(data)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "250a11b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9421661222313225e-30\n"
     ]
    }
   ],
   "source": [
    "final_prob = model.score(words[0])\n",
    "for i in range(1,len(words)):\n",
    "    final_prob = final_prob*model.score(words[i], words[i-1].split())\n",
    "print(final_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72f497a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileOut = \"prob_smooth_\" + inputSentence\n",
    "\n",
    "f = open(fileOut, \"w\")\n",
    "f.write(\"Probability of {} in {}: {}\".format(data, inputSentence, final_prob))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dc96ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'jogador', 'sofreu', 'uma', 'queda', 'no', 'primeiro', 'jogo', '.']\n"
     ]
    }
   ],
   "source": [
    "inputSentence = \"s3.txt\"\n",
    "with open(inputSentence, 'r') as file:\n",
    "    data = file.read().replace('<s>', '').replace('</s>','').replace('\\n', '')\n",
    "\n",
    "words = nltk.word_tokenize(data)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c417fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0366616612862521e-32\n"
     ]
    }
   ],
   "source": [
    "final_prob = model.score(words[0])\n",
    "for i in range(1,len(words)):\n",
    "    final_prob = final_prob*model.score(words[i], words[i-1].split())\n",
    "print(final_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa48b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileOut = \"prob_smooth_\" + inputSentence\n",
    "\n",
    "f = open(fileOut, \"w\")\n",
    "f.write(\"Probability of {} in {}: {}\".format(data, inputSentence, final_prob))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
